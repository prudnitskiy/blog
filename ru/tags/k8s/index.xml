<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>K8s on Инфраструктурный блог</title><link>/ru/tags/k8s/</link><description>Инфраструктурный блог (K8s)</description><generator>Hugo -- gohugo.io</generator><language>ru-ru</language><lastBuildDate>Wed, 16 Feb 2022 20:00:00 +0000</lastBuildDate><atom:link href="/ru/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Как сдать экзамен Cerified Kubernetes Administrator: личный опыт</title><link>/ru/post/2022-02-16-CKA/</link><pubDate>Wed, 16 Feb 2022 20:00:00 +0000</pubDate><guid>/ru/post/2022-02-16-CKA/</guid><description>&lt;p>Экзамен Kubernetes Certified Administrator (CKA) по праву считается самым сложным из &amp;ldquo;базовых&amp;rdquo;. Интернет буквально заполнен жалобами людей, которые заплатили 400$ за попытку и не смогли его сдать. Linux Foundation предлагает одну дополнительную попытку и тестовый вариант экзамена (аж 3 тестовых сессии). Это не помогает все равно - LFS сначала сделали упрощенный вариант CKAD (Certified Kubernetes Application Developer), а потом - еще более упрощенный CKAN. Я этот экзамен сдал. С первой попытки и на 86% (то есть я сделал одну ошибку в одном сложном вопросе). Вся подготовка заняла пол-года (на момент старта знания у меня были только теоретические). В этой статье я расскажу, как я готовился, зачем это вообще надо и на что обращать внимание.&lt;/p>
&lt;h2 id="мотивация" >
&lt;div>
&lt;a href="#%d0%bc%d0%be%d1%82%d0%b8%d0%b2%d0%b0%d1%86%d0%b8%d1%8f">
#
&lt;/a>
Мотивация
&lt;/div>
&lt;/h2>
&lt;p>Kubernetes переживает сейчас бурный рост и его используют очень многие компании. Его ближайший конкурент mesos прекратил свою разработку (потому, что превосходство k8s - подавляющее). Другой конкурент - Hashicorp Nomad - на самом деле не совсем конкурент, так как решает несколько иные вопросы. Состояние Docker Swarm настолько ужасно, что Mirantis не убивает его, по-моему, из чистой жалости &amp;ndash; я ни разу не видел сколько-то работающих конфигураций Swarm, а в тестах он ведет себя настолько нестабильно, что непонятно, зачем он в принципе нужен. Linux Foundation придумали 3 программы сертификации для тех, кто работает с K8s:&lt;/p>
&lt;ul>
&lt;li>Certified Kubernetes Administrator (CKA). Экзамен для тех, кто строит и поддерживает инфраструктуры на базе K8s (operations engineering team)&lt;/li>
&lt;li>Certified Kubernetes Application Developer (CKAD). Экзамен для разработчиков, который проверяет знание best practices по написанию и развертыванию приложений внутри K8s. Этакий CKA на минималках с упором именно на deploy.&lt;/li>
&lt;li>Certified Kubernetes Security (CKS). Частично повторяет CKA, но с упором на безопасность. Для сдачи этого экзамена нужно иметь активный CKA - экзамен частично строится на его программе.&lt;/li>
&lt;li>Certified Kubernetes Associate (CKAN). CKAD на минималках, совсем-совсем базовые вопросы для тех, кому и CKAD слишком сложно (все равно не сдают).&lt;/li>
&lt;/ul>
&lt;p>Все экзамены из &amp;ldquo;проф-линейки&amp;rdquo; (то есть исключая CKAN) стоят по 400$ за попытку, сдать их надо за год с момента оплаты. Каждый экзамен дает право на одну бесплатную пересдачу (в течении того же самого года). Приятная особенность - сдавать экзамены можно дома. За экзамен выдают электронный именной сертификат, который действует три года и очень нравится рекрутерам :)&lt;/p>
&lt;h2 id="процесс-сдачи" >
&lt;div>
&lt;a href="#%d0%bf%d1%80%d0%be%d1%86%d0%b5%d1%81%d1%81-%d1%81%d0%b4%d0%b0%d1%87%d0%b8">
#
&lt;/a>
Процесс сдачи
&lt;/div>
&lt;/h2>
&lt;p>Процесс в целом похож на классический экзамен для получения сертификата. Проводится он на английском языке (для ценителей есть японский и китайский варианты). Экзамен сдается дома с помощью прокторинга - нужно предоставить специальной программе доступ к рабочему столу компьютера и камере, специальный человек по ту сторону экрана (проктор) будет следить, чтобы экзаменуемый не списывал и ему не подсказывали. В целом условия довольно типичные:&lt;/p>
&lt;ul>
&lt;li>Нельзя иметь при себе какие-либо смарт-устройства (телефон, смарт-часы, ноутбук)&lt;/li>
&lt;li>На столе не может быть ничего, кроме, собственно, компа для сдачи&lt;/li>
&lt;li>Напитки разрешены только прозрачные и в прозрачной же таре&lt;/li>
&lt;li>На старте экзамена проктор просит показать два разных ID, одно из которых должно иметь ФИО латиницей (именно так, как оно записано в заявке на экзамен) и фото.&lt;/li>
&lt;li>Комната должна быть пуста и в ней не должно быть других людей&lt;/li>
&lt;li>Запрещены наушники&lt;/li>
&lt;/ul>
&lt;p>Экзамен идет 2 часа (раньше было 3, поменяли в прошлом году) и времени, на самом деле, довольно мало. Многие из сдающих не успевают ответить на все вопросы, я слышал это много раз. Все вопросы (это важная часть) - практические, в экзамене вообще нет теоретических вопросов. Это, на самом деле, один из основных секретов сложности экзамена &amp;ndash; так, как нет теоретических вопросов &amp;ndash; их невозможно списать и заучить. Вместо этого в рамках экзамена выдается тестовое окружение (SSH консоль прямо в окне браузера) и практические задачи что-то сделать: создать/удалить deployment, привязать secret и так далее. Вопросы можно условно поделить на &amp;ldquo;простые&amp;rdquo; и &amp;ldquo;сложные&amp;rdquo;, за &amp;ldquo;сложные&amp;rdquo; дают больше баллов. Именно по этому имеет смысл сначала просмотреть все вопросы. Если на вопрос можно сразу ответить (меньше, чем за минуту) - отвечаем, нет &amp;ndash; идем дальше. После этого имеет смысл уделить время сложным вопросам, чтобы заработать больше баллов. На 2 часа выдается порядка 20 вопросов, то есть это 6 минут на вопрос в среднем. Быстрее ответите на простые вопросы &amp;ndash; больше времени будет на сложные. В процессе экзамена никаких подсказок не будет, балл сразу тоже не покажут &amp;ndash; письмо с оценкой приходит через сутки-двое. На экзамене разрешено использовать официальную документацию kubernetes (с оффсайта), включая справочник по API. Так же можно использовать github проекта (там есть референсные конфигурации, например - готовые конфиги PV/PVC). В принципе можно использовать и встроенные механизмы помощи (&lt;code>kubectl explain&lt;/code>, &lt;code>kubectl api-resources&lt;/code>). Ничего другого (google, kubernetes discussion forum, SO) - использовать нельзя, это кончается дисквалификацией. Официальное руководство разрешает сделать необходимые закладки заранее &amp;ndash; этим обязательно надо пользоваться.&lt;/p>
&lt;h2 id="подготовка" >
&lt;div>
&lt;a href="#%d0%bf%d0%be%d0%b4%d0%b3%d0%be%d1%82%d0%be%d0%b2%d0%ba%d0%b0">
#
&lt;/a>
Подготовка
&lt;/div>
&lt;/h2>
&lt;p>Сдать CKA без подготовки невозможно. Нет, возможно, конечно, если вас зовут Келси Хайтауэр, Андрей Квапил или Бильям Ибрагим (вот насчет последнего не уверен). Так, как экзамен практический &amp;ndash; нужно тренировать именно практические вопросы и тренировать их на скорость:&lt;/p>
&lt;ul>
&lt;li>создание-изменение-скейлинг deployment-ов.&lt;/li>
&lt;li>редактура разных компонентов (apply/inline edit/patch). Нужно держать в памяти иммутабельные поля, чтобы не споткнуться на экзамене.&lt;/li>
&lt;li>создание-редактирование сервисов.&lt;/li>
&lt;li>создание-редактирование pod-ов. Последнее может пригодиться для отладки, когда нужно посмотреть на кластер &amp;ldquo;изнутри&amp;rdquo; и проще поднять busybox или netshot.&lt;/li>
&lt;li>создание configmap-ов и secret-ов, они очень часто попадаются.&lt;/li>
&lt;/ul>
&lt;p>Очень рекомендую запомнить и потренировать императивный подходы к вышеперечисленному, чтобы не писать длинный yaml с нуля. Это еще помогает быстро создать &amp;ldquo;скелет&amp;rdquo; конфигурации примерно таким образом: &lt;code>kubectl create deploy nginx --replicas=2 --image=nginx:stable -o yaml --dry-run=client &amp;gt; nginx-deploy.yaml&lt;/code>&lt;/p>
&lt;p>Я очень рекомендую пройтись по списку всех основных примитивов (по документации или книге) и понять, чем, к примеру, отличается service от ingress а daemonset от statefulset. Так же очень советую сделать лабораторную работу &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes hard way&lt;/a>. Вам абсолютно точно не придется делать этого на экзамене - это слишком долго. Но эта задача позволит понять, из каких компонент K8s состоит, как они взаимодействуют друг с другом и что происходит, когда они ломаются. Для прогона практики можно арендовать сервер в Linode (новоприбывшим дают 100$ кредита на 2 месяца) или в больших облаках (там тоже есть бонус для новичков, главное &amp;ndash; следить за балансом). Машины с 4Gb RAM хватит для большинства лабораторных (исключая те, что про CNI - там нужно две машины в одной сети). Практика - 70% успеха на этом экзамене.&lt;/p>
&lt;p>Если душа лежит к видеокурсам &amp;ndash; обязательно берите те, где есть практические задачи (hands-on labs). Мне в свое время помог &lt;a href="https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests">курс&lt;/a> от Мумшада Манамбета (но один курс вас не спасет, практика обязательна и тут она важнее теории).&lt;/p>
&lt;p>Условия экзамена разрешают использовать закладки - и их нужно обязательно подготовить. Я готовил закладки сам и поделил их на 3 группы:&lt;/p>
&lt;ul>
&lt;li>справочники и прочая теория (API, описание certifications API, CNI, CRI).&lt;/li>
&lt;li>практические задачи из официальной документации. В стиле &amp;ldquo;как обновить k8s через kubeadm на специфичную версию&amp;rdquo;, &amp;ldquo;как создать и затем использовать секрет&amp;rdquo;&lt;/li>
&lt;li>готовые референсные конфиги (yaml с готовым рабочим примитивом).&lt;/li>
&lt;/ul>
&lt;p>В сети есть некоторое количество уже готовых подборок закладок, вот &lt;a href="https://gist.github.com/prudnitskiy/e71c819743aaea142b1ae58bd4d862b3">эта&lt;/a> подборка от ITNext мне показалась лучшей.&lt;/p>
&lt;p>В рамках подготовки к экзамену, если он уже оплачен &amp;ndash; появляется возможность взять тестовые сессии на [killer.sh]. Бесплатно доступно 3 сессии, каждая такая сессия доступна сутки с момента запроса. По моим ощущениям - эти сессии слегка сложнее реального экзамена, но не радикально. Эти сессии помогут понять, как идет реальный экзамен (там даже интерфейс такой же, только проктора нет).&lt;/p>
&lt;h2 id="чего-я-делать-не-советую" >
&lt;div>
&lt;a href="#%d1%87%d0%b5%d0%b3%d0%be-%d1%8f-%d0%b4%d0%b5%d0%bb%d0%b0%d1%82%d1%8c-%d0%bd%d0%b5-%d1%81%d0%be%d0%b2%d0%b5%d1%82%d1%83%d1%8e">
#
&lt;/a>
Чего я делать не советую
&lt;/div>
&lt;/h2>
&lt;p>Первое и самое главное &amp;ndash; сильно не советую жульничать. Экзамен сдается через Pearson Vue, в наихудшем случае это кончится баном в PV и аннуляцией уже выданных сертификатов (да, их EULA такое позволяет). Экзамен непростой, но если озаботиться подготовкой &amp;ndash; сдать его вполне реально. Многие статьи рекомендуют выучить hotkey-и для tmux и идеально владеть vim. ИМХО, если вы не умеете &amp;ndash; тратить время не стоит. Если vim вам прямо совсем не дается &amp;ndash; на тестовой машине есть nano, нужно просто знать, как переключить редактор (через переменную &lt;code>EDITOR&lt;/code>). Tmux вообще не нужен на экзамене. Так же не советую пытаться что-то запмнить наизусть &amp;ndash; официальная документация и подсказки из API всегда доступны. Тратить силы и время на именно запоминание каких-то цифр или команд &amp;ndash; просто расточительство и никак не поможет.&lt;/p>
&lt;p>Не советую назначать экзамен после окончания рабочего дня или в сильно неудобное время &amp;ndash; экзамен требует полной сосредоточенности, отвлечение может стоить вам баллов, и, как следствие, сертификата.&lt;/p>
&lt;h2 id="на-что-обращать-внимание-на-экзамене" >
&lt;div>
&lt;a href="#%d0%bd%d0%b0-%d1%87%d1%82%d0%be-%d0%be%d0%b1%d1%80%d0%b0%d1%89%d0%b0%d1%82%d1%8c-%d0%b2%d0%bd%d0%b8%d0%bc%d0%b0%d0%bd%d0%b8%d0%b5-%d0%bd%d0%b0-%d1%8d%d0%ba%d0%b7%d0%b0%d0%bc%d0%b5%d0%bd%d0%b5">
#
&lt;/a>
На что обращать внимание на экзамене
&lt;/div>
&lt;/h2>
&lt;p>Главное &amp;ndash; на &lt;em>время&lt;/em>. Его &lt;em>очень мало&lt;/em> и важность времени трудно переоценить. Если вы чувствуете, что застряли в задаче &amp;ndash; поставьте флажок и двигайтесь дальше. Если будет время &amp;ndash; потом вернетесь. Следующее, на что я советую обращать внимание &amp;ndash; на формулировку задачи. В формулировке есть все необходимое, но там может скрываться важная подсказка, которую легко упустить: указание на определенный namespace, label, граничное условие. Авторы тестов такие вопросы очень любят, проколоться элементарно. Результат? Незачет вопроса.&lt;/p>
&lt;p>Если вы ответили на все вопросы, а время еще осталось (хотя бы минут 5-10) &amp;ndash; пройдитесь по вопросам еще раз, прочитайте формулировки и проверьте решение. Одну свою задачу я переделывал, ибо поторопился. Простые задачи в целом довольно прямолинейные и если вы уделили время практике &amp;ndash; проблем быть не должно - развернуть сервис, написать политику безопасности, дать пользователю права, привязать-отвязать диск, организовать двум сервисам общение друг с другом, разобраться, почему сервис не стартует&amp;hellip; Нормальные задачи нормального админа. Сложность у задач, как я уже говорил, разная. Сложная задача видна сразу, уже по формулировкам понятно, что она даст много баллов. Если задачу можно решить несколькими способами &amp;ndash; они все идут в зачет (проверяется результат, а не способ). Но опять же, проверяйте условия задачи!&lt;/p>
&lt;h2 id="заключение" >
&lt;div>
&lt;a href="#%d0%b7%d0%b0%d0%ba%d0%bb%d1%8e%d1%87%d0%b5%d0%bd%d0%b8%d0%b5">
#
&lt;/a>
Заключение
&lt;/div>
&lt;/h2>
&lt;p>Не смотря на страшный имидж - экзамен совсем не такой сложный, каким хочет казаться. Низкий процент успеха продиктован, думаю, физическим отсутствием дампов (теоретических вопросов-то нет!). При наличии времени на подготовку и некоторых теоретических знаний о том, как работает Linux сдать его будет не смертельно сложно. Главное - следить за временем, быть внимательным и потратить время на подготовку. Экзамен покрывает разные топики и человек, который его сдал - скорее всего знает k8s на приличном уровне. Даже сама подготовка - хорошая ревизия знаний и умений в этой области.&lt;/p>
&lt;p>Удачи на экзамене!&lt;/p></description></item><item><title>Распределяем pod-ы по машинам в kubernetes</title><link>/ru/post/2021-01-15-k8s-pod-distribution/</link><pubDate>Fri, 15 Jan 2021 08:16:22 +0000</pubDate><guid>/ru/post/2021-01-15-k8s-pod-distribution/</guid><description>&lt;p>Kubernetes иногда называют &amp;ldquo;операционной системой для дата-центров&amp;rdquo; &amp;ndash; и в этом есть логика. K8s позволяет представить группу серверов (условный ЦОД) как единое вычислительное пространство. Оператор просто бросает задания в K8s, а тот сам выбирает, где тот или иной контейнер лучше разместить. Чаще всего делает это он хорошо. Но иногда появляется необходимость как-то управлять этим процессом. Об этом я и расскажу.&lt;/p>
&lt;h2 id="зачем-управлять-распределением-pod-ов" >
&lt;div>
&lt;a href="#%d0%b7%d0%b0%d1%87%d0%b5%d0%bc-%d1%83%d0%bf%d1%80%d0%b0%d0%b2%d0%bb%d1%8f%d1%82%d1%8c-%d1%80%d0%b0%d1%81%d0%bf%d1%80%d0%b5%d0%b4%d0%b5%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5%d0%bc-pod-%d0%be%d0%b2">
#
&lt;/a>
Зачем управлять распределением POD-ов?
&lt;/div>
&lt;/h2>
&lt;p>Зачем вообще нужно привязывать поды к определенным узлам? Это может быть связано с производительностью, безопасностью или надежность. Например &amp;ndash; pod может требовать доступ к специфическому железу (видеокарты и ML-ускорители для задач машинного обучения, аппаратные криптоускорители). Это может быть продиктовано безопасностью: критические части проекта будут размещаться на машинах, где физически не может быть ничего, кроме них. Это снижает шансы на то, что удачный взлом, скажем, сервиса регистраций раскроет данные о платежах. Некоторые стандарты безопасности (включая PCI DSS) имеют даже требования к физической безопасности серверов &amp;ndash; датчики вскрытия, пломбы на корпусках, запрет на доступ. Отдельная удобная особенность &amp;ndash; tier-инг. Нагрузку в кластере можно разделить на &amp;ldquo;важную&amp;rdquo; и &amp;ldquo;не очень&amp;rdquo;. Под важную выделять мощные современные машины с резервированием PSU, горячей замены дисков и памяти, под &amp;ldquo;не очень&amp;rdquo; &amp;ndash; соскрести какой-нибудь хлам. В облаках это делается даже проще за счет spot instances. Такие инстансы дешевле (порой радикально), но их работу никто не гарантирует &amp;ndash; инстанс может отключится в любой момент (вместо него появится новый). Это вызовет пересоздание POD-ов, но для чего-то маловажного это, может &amp;ndash; и не страшно совсем.&lt;/p>
&lt;h2 id="способы-управления" >
&lt;div>
&lt;a href="#%d1%81%d0%bf%d0%be%d1%81%d0%be%d0%b1%d1%8b-%d1%83%d0%bf%d1%80%d0%b0%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d1%8f">
#
&lt;/a>
Способы управления
&lt;/div>
&lt;/h2>
&lt;h3 id="nodeselector" >
&lt;div>
&lt;a href="#nodeselector">
##
&lt;/a>
NodeSelector
&lt;/div>
&lt;/h3>
&lt;p>Это самый простой способ управления аллокацией. Он предельно прямолинеен &amp;ndash; запутаться в нем невозможно. Выполняется в 2 этапа. Сначала надо поставить метки на node командой label:&lt;/p>
&lt;pre>&lt;code>kubectl label nodes snowflake3 disk=hdd
&lt;/code>&lt;/pre>
&lt;p>Проверить, какие метки уже есть можно через &lt;code>kubectl describe nodes&lt;/code>&lt;/p>
&lt;p>Теперь можно указать pod-у требование на привязку к конкретной метке. Для аллокации пода будут использоваться только помеченые узлы, то есть при включении nodeSelector для пода ноды без меток будут игнорироваться:&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: nginx
labels:
env: test
spec:
containers:
- name: nginx
image: nginx
nodeSelector:
disk: hdd
&lt;/code>&lt;/pre>
&lt;p>Для deployment nodeSelector передается в шаблон pod-а, как обычно. Не смотря на удобство и прямолинейность подхода &amp;ndash; nodeSelector имеет три минуса:&lt;/p>
&lt;ul>
&lt;li>nodeSelector применяется в момент аллокации пода и бесполезен, если под уже аллоцирован. Если вам нужно &amp;ldquo;освободить&amp;rdquo; ноду &amp;ndash; придется поставить на нее метку и затем выкинуть оттуда поды командой &lt;code>drain&lt;/code>&lt;/li>
&lt;li>nodeSelector не особенно гибкий и работает по принципу &amp;ldquo;один к одному&amp;rdquo;. К примеру, можно сделать метки для машин &lt;code>small&lt;/code>, &lt;code>medium&lt;/code> и &lt;code>large&lt;/code> и указать поду, что он должен развернуться на машине класса &lt;code>small&lt;/code>. Но нельзя &amp;ndash; на машине класса &lt;code>medium&lt;/code> или &lt;code>large&lt;/code> &amp;ndash; возможен только один вариант.&lt;/li>
&lt;li>nodeSelector не запрещает аллокаций. То есть на машине с меткой могут размещаться поды без nodeAffinity. Для решения этой проблемы придуман иной подход.&lt;/li>
&lt;/ul>
&lt;h3 id="taints-and-tolerations" >
&lt;div>
&lt;a href="#taints-and-tolerations">
##
&lt;/a>
Taints and Tolerations
&lt;/div>
&lt;/h3>
&lt;p>Taints &amp;ndash; это NodeAffinity наоборот. Если nodeAffinity говорит scheduler-у, где он должен размещать pod-ы, то taint говорит, где pod-ы размещать &lt;em>нельзя&lt;/em>. Любой taint запрещает размещение на машине любых подов (есть одно исключение, про него дальше). Однако можно создать под, который будет игнорировать (&lt;code>tolerate&lt;/code>) этот запрет &amp;ndash; и данный pod запустится на данной машине. Даже если у вас есть совершенно пустой нормальный кластер kubernetes &amp;ndash; у вас уже есть taint. По умолчанию kubernetes запрещает размещать обычные поды на master nodes &amp;ndash; это taint &lt;code>node-role.kubernetes.io/master&lt;/code>&lt;/p>
&lt;p>taint создается с помощью команды &lt;code>kubectl taint&lt;/code>. Общий вид:&lt;/p>
&lt;pre>&lt;code>kubectl taint nodes nodeName taintKey=taintValue:taintEffect
&lt;/code>&lt;/pre>
&lt;p>taintKey и taintValue &amp;ndash; это просто метки, они могут быть произвольными. У taintEffect есть 3 возможных значения:&lt;/p>
&lt;ul>
&lt;li>NoSchedule &amp;ndash; новые поды не будут аллоцироваться, однако существующие продолжат свою работу&lt;/li>
&lt;li>PreferNoSchedule &amp;ndash; новые поды не будут аллоцироваться, если в кластере есть свободное место&lt;/li>
&lt;li>NoExecute &amp;ndash; все запущенные поды без tolerations должны быть убраны&lt;/li>
&lt;/ul>
&lt;p>Теперь о том, как прописываются tolerations. Язык tolerations слегка сложнее прямолинейного подхода nodeAffinity:&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: nginx
labels:
env: test
spec:
containers:
- name: nginx
image: nginx
tolerations:
- key: &amp;quot;pft-env&amp;quot;
operator: &amp;quot;Exists&amp;quot;
effect: &amp;quot;NoSchedule&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>в данном примере мы создадим под, который будет игнорировать taint, созданный вот такой командой:&lt;/p>
&lt;pre>&lt;code>kubectl taint nodes pft-node-1 pft-env=true:NoSchedule
&lt;/code>&lt;/pre>
&lt;p>Есть более сложный вариант &amp;ndash; можно учитывать не только факт наличия метки, но и ее значение. Создадим пару taint-ов:&lt;/p>
&lt;pre>&lt;code>kubectl taint nodes secure-1 secGroup=secure:NoSchedule
kubectl taint nodes insecure-2 secGroup=unsafe:NoSchedule
apiVersion: v1
kind: Pod
metadata:
name: vault
labels:
env: test
spec:
containers:
- name: vault
image: vault
tolerations:
- key: &amp;quot;secGroup&amp;quot;
operator: &amp;quot;Equals&amp;quot;
value: &amp;quot;secure&amp;quot;
effect: &amp;quot;NoSchedule&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>В данном примере pod vault будет создан только на ноде secure-1, потому что только на ней &lt;code>secGroup&lt;/code> равен &lt;code>secure&lt;/code>.&lt;/p>
&lt;p>Taint-ов можно создать сколь угодно много и условия проверки можно сочетать, как в примере ниже:&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: processing
labels:
env: test
spec:
containers:
- name: processing
image: processing
tolerations:
- key: &amp;quot;dedicatedNode&amp;quot;
operator: &amp;quot;Exists&amp;quot;
effect: &amp;quot;NoSchedule&amp;quot;
- key: &amp;quot;secGroup&amp;quot;
operator: &amp;quot;Equals&amp;quot;
value: &amp;quot;secure&amp;quot;
effect: &amp;quot;NoExecute&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>В данном примере мы выделяем пул выделенных машин taint-ом &amp;ldquo;dedicatedNode&amp;rdquo; и отдельно помечаем группу максимальной безопасности значением secure для группы secGroup.&lt;/p>
&lt;p>Удалить taint можно, добавив в конец знак минуса:&lt;/p>
&lt;pre>&lt;code>kubectl taint nodes secure-1 secGroup=secure:NoSchedule-
&lt;/code>&lt;/pre>
&lt;h3 id="nodeaffinity" >
&lt;div>
&lt;a href="#nodeaffinity">
##
&lt;/a>
nodeAffinity
&lt;/div>
&lt;/h3>
&lt;p>Не смотря на простоту и эффективность механизма nodeSelector &amp;ndash; механизм это прямолинейный и не особенно гибкий. Авторы kubernetes предлагают более мощный, гибкий (а так же &amp;ndash; сложный и неудобный) механизм &amp;ndash; nodeAffinity. Язык описания nodeAffinity предлагает несколько мощных возомжностей:&lt;/p>
&lt;ul>
&lt;li>логические операторы для выбора условия размещения &amp;ndash; IN (размещать на одной из нод с разными метками) или AND (размещать на нодах, имеющих обе метки сразу)&lt;/li>
&lt;li>можно выбраить политики размещения pod-ов относительно друг друга: например &amp;ndash; запретить экземплярам кэша оказываться на одной физической машине или требовать размещение приложения вместе с экземпляром кеша на одном физическом узле&lt;/li>
&lt;/ul>
&lt;p>Минус nodeAffinity в том, что язык очень многословный и читается тяжело. Общая спецификация выглядит так:&lt;/p>
&lt;pre>&lt;code>spec:
affinity:
nodeAffinity:
{affinityClass}:
nodeSelectorTerms:
- matchExpressions:
- key: {affinityKey}
operator: {affinityOperator}
values:
- {affinityValues}
&lt;/code>&lt;/pre>
&lt;p>affinityClass влияет на строгость выбора узла:&lt;/p>
&lt;ul>
&lt;li>requiredDuringSchedulingIgnoredDuringExecution: обязательно размещать pod-ы по требованию nodeAffinity. Если разместить не получится &amp;ndash; pod застрянет в статусе Pending&lt;/li>
&lt;li>preferredDuringSchedulingIgnoredDuringExecution: по возможности размещать pod-ы по требованиям affinity. Если поды не влезли &amp;ndash; scheduler разместит их &amp;ldquo;как получится&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>affinityKey &amp;ndash; это метка (ключ), по которой мы будем искать ноды для размещения pod-ов.
affinityValues &amp;ndash; это значения метки, которые нам подойдут
affinityOperator &amp;ndash; это тот логический оператор, по которому будет производится выбор метки. Варианты:&lt;/p>
&lt;ul>
&lt;li>In &amp;ndash; подойдет любое из перечисленных значений&lt;/li>
&lt;li>NotIn &amp;ndash; противоположно In&lt;/li>
&lt;li>Exists &amp;ndash; метка просто есть (values игнорируется)&lt;/li>
&lt;li>DoesNotExists &amp;ndash; противоположно Exists&lt;/li>
&lt;li>Gt &amp;ndash; Greater than &amp;ndash; значение метки больше указанного в политике числа. Сработает только для чисел&lt;/li>
&lt;li>Lt &amp;ndash; Less than &amp;ndash; противоположно Gt&lt;/li>
&lt;/ul>
&lt;p>affinityClass preferredDuringSchedulingIgnoredDuringExecution слегка отличается &amp;ndash; вместо nodeSelectorTerms используется поле preference (синтаксис такой же), плюс есть обязательное поле weight &amp;ndash; оно отвечает за приоритет при выборе node.&lt;/p>
&lt;p>Пример:&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: nginx-with-node-affinity
spec:
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms:
- matchExpressions:
- key: kubernetes.io/e2e-az-name
operator: In
values:
- e2e-az1
- e2e-az2
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 1
preference:
matchExpressions:
- key: kubernetes.io/node-tier
operator: In
values:
- silver
- bronze
containers:
- name: with-node-affinity
image: k8s.gcr.io/nginx
&lt;/code>&lt;/pre>
&lt;p>affinity не учитывается для уже аллоцированных nodes, так что если нужно освободить node-у от всех подов которые там уже есть &amp;ndash; поможет команда &lt;code>kubectl node drain&lt;/code>&lt;/p>
&lt;h4 id="лирическое-отступление----podaffinitty-и-podantiaffinitty" >
&lt;div>
&lt;a href="#%d0%bb%d0%b8%d1%80%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%be%d0%b5-%d0%be%d1%82%d1%81%d1%82%d1%83%d0%bf%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5----podaffinitty-%d0%b8-podantiaffinitty">
###
&lt;/a>
Лирическое отступление &amp;ndash; PodAffinitty и PodAntiAffinitty
&lt;/div>
&lt;/h4>
&lt;p>Механизм, который помогает размещать pod-ы относительно нод &amp;ndash; может так же помочь и разместить pod-ы относительно друг друга &amp;ndash; за это отвечают классы PodAffinity и PodAntiAffinity. Все три класса можно сочетать друг с другом, синтаксис внутри одинаковый, по этому просто покажу пример:&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: redis-cache
spec:
selector:
matchLabels:
app: store
replicas: 3
template:
metadata:
labels:
app: store
spec:
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- store
topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;
containers:
- name: redis-server
image: redis:3.2-alpine
&lt;/code>&lt;/pre>
&lt;p>В этом примере мы запрещаем экземплярам redis размещаться на одном узле. Каждый pod в этом deployment получит метку &lt;code>app:store&lt;/code>, политика podAntiAffinity запрещает размещать второй под с меткой app=store на ноде с таким же hostname. Важный параметр тут &amp;ndash; topologyKey. Именно по нему scheduler понимает, какие node-ы считаются одной зоной размещения,а какие &amp;ndash; нет. Усложним пример, добавив web worker:&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: web-server
spec:
selector:
matchLabels:
app: web-store
replicas: 3
template:
metadata:
labels:
app: web-store
spec:
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- web-store
topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- store
topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;
containers:
- name: web-app
image: nginx:1.16-alpine
&lt;/code>&lt;/pre>
&lt;p>В этом примере мы размещаем nginx на разных node (как мы сделали с redis), но при этом требуем, чтобы nginx размещался вместе с redis. Это может быть удобно для кешей. Проверим, что получилось:&lt;/p>
&lt;pre>&lt;code>&amp;gt; kubectl get pods -o wide
NAME READY STATUS RESTARTS AGE IP NODE
redis-cache-1450370735-6dzlj 1/1 Running 0 8m 10.192.4.2 kube-node-3
redis-cache-1450370735-j2j96 1/1 Running 0 8m 10.192.2.2 kube-node-1
redis-cache-1450370735-z73mh 1/1 Running 0 8m 10.192.3.1 kube-node-2
web-server-1287567482-5d4dz 1/1 Running 0 7m 10.192.2.3 kube-node-1
web-server-1287567482-6f7v5 1/1 Running 0 7m 10.192.4.3 kube-node-3
web-server-1287567482-s330j 1/1 Running 0 7m 10.192.3.2 kube-node-2
&lt;/code>&lt;/pre>
&lt;h3 id="static-pod-allocations" >
&lt;div>
&lt;a href="#static-pod-allocations">
##
&lt;/a>
Static pod allocations
&lt;/div>
&lt;/h3>
&lt;p>Это очень редкий случай, но не упомянуть его было бы нечестно. Pod-ы можно аллоцировать полностью статически, вручную привязав к конкретной node. В этом случае scheduler никак на них не влияет. На них не действуют taints, nodeSelector и podAffinity. Даже node drain ничего не сможет с такими подами сделать. Зачем это может потребоваться? Ну, во-первых для запуска таких pod-ов не нужен работающий scheduler или apiserver. Это делает размещение таких подов сверхнадежным &amp;ndash; они будут работать всегда. Именно так kubeadm устанавливает свои компоненты &amp;ndash; это не полноценные демоны, а контейнеры, которые вручную привязаны к master node.&lt;/p>
&lt;p>Во-вторых такой подход может потребоваться в случае, если какой-то контейнер надо привязать к конкретной, строго определенной node вручную и ни при каких условиях не давать ему оттуда уезжать. Скажем, у вас какое-то особое шифрование и оно зависит от HSM, который физически подключен к определенной, особо защищенной машине. Вообще &amp;ndash; это порочная практика и такой сценарий лучше решается через nodeSelector + taint, но мало ли?&lt;/p>
&lt;p>Выполнить статическую аллоакцию очень просто &amp;ndash; нужно положить манифесты pod-ов в папку со статическими подами. Этот путь можно задать двумя путями:&lt;/p>
&lt;ul>
&lt;li>через аргумент командной строки kubelet: &lt;code>--pod-manifest-path&lt;/code>&lt;/li>
&lt;li>через параметр конфига &lt;code>staticPodPath&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Если у вас kubernetes установлен через kubeadm &amp;ndash; этот параметр там уже есть, kubelet будет искать статические манифесты по адресу &lt;code>/etc/kubernetes/manifests&lt;/code>. Kubelet перечитывает папку с манифестами каждые 10 секунд. Если удалить манифест &amp;ndash; kubernetes удалит pod.&lt;/p>
&lt;p>Просто создадим манифест статического пода&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF &amp;gt;/etc/kubernetes/manifests/static-web.yaml
apiVersion: v1
kind: Pod
metadata:
name: static-web
labels:
role: static
spec:
containers:
- name: web
image: nginx
ports:
- name: web
containerPort: 80
protocol: TCP
EOF
&lt;/code>&lt;/pre>
&lt;p>И проверим, что получилось:&lt;/p>
&lt;pre>&lt;code>&amp;gt; kubectl get pods -l role=static
NAME READY STATUS RESTARTS AGE
static-web-my-node1 1/1 Running 0 2m
&lt;/code>&lt;/pre>
&lt;h2 id="порядок-применения" >
&lt;div>
&lt;a href="#%d0%bf%d0%be%d1%80%d1%8f%d0%b4%d0%be%d0%ba-%d0%bf%d1%80%d0%b8%d0%bc%d0%b5%d0%bd%d0%b5%d0%bd%d0%b8%d1%8f">
#
&lt;/a>
Порядок применения
&lt;/div>
&lt;/h2>
&lt;p>Первым всегда применяется static pod. Он игнорирует все (taints, affinities, node selectors).&lt;/p>
&lt;p>Вторым по списку применяется taint. Если у pod нет toleration &amp;ndash; он не будет размещен, по этому taint &amp;ndash; это очень эффективный способ &amp;ldquo;разогнать&amp;rdquo; pod-ы с определенного узла (или группы узлов).&lt;/p>
&lt;p>В случае, если есть nodeAffinity и nodeSelector &amp;ndash; должны сработать оба условия сразу (то есть &amp;ndash; и метка селектора и условия affinity).&lt;/p>
&lt;h2 id="заключение" >
&lt;div>
&lt;a href="#%d0%b7%d0%b0%d0%ba%d0%bb%d1%8e%d1%87%d0%b5%d0%bd%d0%b8%d0%b5">
#
&lt;/a>
Заключение
&lt;/div>
&lt;/h2>
&lt;p>Kubernetes &amp;ndash; мощный, богатый на возможности инструмент. Он кажется слегка неудобным, но ровно до момента понимания логики его работы. Scheduler у kubernetes практически ключевой компонент, и он достаточно гибок, пусть и не самым лучшим образом описан. Надеюсь &amp;ndash; эта статья кому-то поможет. Высокого вам аптайма!&lt;/p></description></item></channel></rss>