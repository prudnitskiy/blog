<!doctype html><html lang=ru-ru data-theme>
<head>
<meta charset=utf-8>
<meta name=HandheldFriendly content="True">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=referrer content="no-referrer-when-downgrade">
<title>Миграция на mongo replica set без потери данных - SRE Blog</title>
<meta name=description content="По сути - это не статья, а, скорее, просто памятка, чтобы самому не забыть. MongoDB - популярный документо-ориентированный движок управления базой. Штатно он имеет две разных технологии кластеризации: репликационные наборы (replica set) и шардирование (shard). Разница проста - в случае реплики на всех узлах данные одинаковы и любой узел может выступать источником данных (внимание, mongodb не является CAP-полной базой, так что точность данных тут под большим вопросом!), что обеспечивает отказоустойчивость. В случае шардирования данные &ldquo;размазаны&rdquo; по всему набору шарда, но каждый сервер внутри шарда имеет только свои данные. За счет этого распределяется нагрузка (например, с трех узлов данные можно читать параллельно), но снижается надежность - упавший узел означает потерю части данных шарда. В данной статье будет описание, как переехать с единичной монги на репсет не потеряв при этом данные.">
<link rel=icon type=image/x-icon href=https://prudnitskiy.pro/favicon.ico>
<link rel=apple-touch-icon-precomposed href=https://prudnitskiy.pro/favicon.png>
<style>body{visibility:hidden;opacity:0}</style>
<noscript>
<style>body{visibility:visible;opacity:1}</style>
</noscript>
<link rel=stylesheet href=https://prudnitskiy.pro/css/style.min.7805c73eb1fdd7219cadcad861e932566ed8a62e3c55386d0058c3967e8ab5d0.css integrity="sha256-eAXHPrH91yGcrcrYYekyVm7Ypi48VThtAFjDln6KtdA=">
<script src=https://prudnitskiy.pro/js/script.min.510c781c39dbb21b4c76d85c82e2bdf4689220adbb7cd61e49e9d293e442fb42.js type=text/javascript integrity="sha256-UQx4HDnbshtMdthcguK99GiSIK27fNYeSenSk+RC+0I="></script>
<meta property="og:title" content="Миграция на mongo replica set без потери данных">
<meta property="og:description" content="По сути - это не статья, а, скорее, просто памятка, чтобы самому не забыть. MongoDB - популярный документо-ориентированный движок управления базой. Штатно он имеет две разных технологии кластеризации: репликационные наборы (replica set) и шардирование (shard). Разница проста - в случае реплики на всех узлах данные одинаковы и любой узел может выступать источником данных (внимание, mongodb не является CAP-полной базой, так что точность данных тут под большим вопросом!), что обеспечивает отказоустойчивость. В случае шардирования данные &ldquo;размазаны&rdquo; по всему набору шарда, но каждый сервер внутри шарда имеет только свои данные. За счет этого распределяется нагрузка (например, с трех узлов данные можно читать параллельно), но снижается надежность - упавший узел означает потерю части данных шарда. В данной статье будет описание, как переехать с единичной монги на репсет не потеряв при этом данные.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://prudnitskiy.pro/post/2015-09-06-mongo-shard/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2015-09-06T00:13:48+00:00">
<meta property="article:modified_time" content="2015-09-06T00:13:48+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Миграция на mongo replica set без потери данных">
<meta name=twitter:description content="По сути - это не статья, а, скорее, просто памятка, чтобы самому не забыть. MongoDB - популярный документо-ориентированный движок управления базой. Штатно он имеет две разных технологии кластеризации: репликационные наборы (replica set) и шардирование (shard). Разница проста - в случае реплики на всех узлах данные одинаковы и любой узел может выступать источником данных (внимание, mongodb не является CAP-полной базой, так что точность данных тут под большим вопросом!), что обеспечивает отказоустойчивость. В случае шардирования данные &ldquo;размазаны&rdquo; по всему набору шарда, но каждый сервер внутри шарда имеет только свои данные. За счет этого распределяется нагрузка (например, с трех узлов данные можно читать параллельно), но снижается надежность - упавший узел означает потерю части данных шарда. В данной статье будет описание, как переехать с единичной монги на репсет не потеряв при этом данные.">
</head>
<body>
<a class=skip-main href=#main>Skip to main content</a>
<div class=container>
<header class=common-header>
<div class=header-top>
<h1 class=site-title>
<a href=/>SRE Blog</a>
</h1>
<ul class=social-icons>
<li>
<a href=https://github.com/prudnitskiy title=Github rel=me>
<span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span>
</a>
</li>
<li>
<a href=https://www.linkedin.com/in/prudnitskiy title=Linkedin rel=me>
<span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span>
</a>
</li>
</ul>
</div>
<nav>
<a href=https://prudnitskiy.pro/about/ title=About>About</a>
<a href=https://prudnitskiy.pro/ title=Blog>Blog</a>
</nav>
</header>
<main id=main tabindex=-1>
<article class="post h-entry">
<div class=post-header>
<header>
<h1 class="p-name post-title">Миграция на mongo replica set без потери данных</h1>
</header>
</div>
<div class="content e-content">
<p>По сути - это не статья, а, скорее, просто памятка, чтобы самому не забыть. MongoDB - популярный документо-ориентированный движок управления базой. Штатно он имеет две разных технологии кластеризации: репликационные наборы (replica set) и шардирование (shard). Разница проста - в случае реплики на всех узлах данные одинаковы и любой узел может выступать источником данных (<strong>внимание, mongodb не является CAP-полной базой, так что точность данных тут под большим вопросом!</strong>), что обеспечивает отказоустойчивость. В случае шардирования данные &ldquo;размазаны&rdquo; по всему набору шарда, но каждый сервер внутри шарда имеет только свои данные. За счет этого распределяется нагрузка (например, с трех узлов данные можно читать параллельно), но снижается надежность - упавший узел означает потерю части данных шарда. В данной статье будет описание, как переехать с единичной монги на репсет не потеряв при этом данные.</p>
<p>Важной особенностью репсета является тот факт, что в нем всегда только один мастер, и запись можно производить только на мастер. Таким забавным способом mongo борется с потенциальными конфликтами конкурентной записи данных (задача, на самом деле, куда как непростая, Riak это ярко демонстрирует). В случае, если мастер умер - производятся выборы нового мастера. К слову, пока они идут - в репсет нельзя записать ничего, от слова совсем.</p>
<h2 id=инструкция>Инструкция
<span>
<a href=#%d0%b8%d0%bd%d1%81%d1%82%d1%80%d1%83%d0%ba%d1%86%d0%b8%d1%8f><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h2><p>Итак, у нас есть одна MongoDB, к которой подключено приложение и мы хотим перевести это приложение на работу с кластером. Авторы mongo рекомендуют иметь нечетное количество узлов в наборе для гарантии достижения кворума, то есть потребуется 3 сервера. В отличае от MySQL - управление выбором узлов для чтения и записи в монго возложено на драйвер (то есть - коннектор языка программирования к базе, а не на саму базу). Перед началом работ очень рекомендую, умеет ли ваш драйвер работать с репсетами. Первое, что нужно поменять - это в файле настроек указать IP адрес, на который mongo привязывается (она должна быть доступна по сети) и задать параметр replicationSet. Один инстанс mongo может одновременно находится только в одном replication set. Параметр - не динамический, mongo надо перезапустить для его применения. Вписываем в /etc/mongo:</p>
<pre><code>#имя replica set. Должно быть уникальным, но единым для всего набора
replSet=replica1
#bind_ip закомментирован - слушать на всех доступных адресах.
#bind_ip=
port=27017
</code></pre>
<p>Перезапускаем mongo, подключаемся к консоли (команда mongo) и инициируем набор:</p>
<pre><code># service mongodb restart
# mongo
&gt; rs.initiate()
{
        &quot;info2&quot; : &quot;no configuration explicitly specified -- making one&quot;,
        &quot;me&quot; : &quot;db1.cluser:27017&quot;,
        &quot;ok&quot; : 1
}
</code></pre>
<p>Чтобы быть уверенными, что мастером останется именно тот сервер, с которого мы начинаем конфигурирование (и на котором храним данные, которые не хотим потерять) - повысим ему приоритет в наборе. Мастер выбирается по принципу - случайный из максимального доступного приоритета, приоритет 0 будет означать, что данный сервер стать мастером не сможет никогда.</p>
<pre><code>replica1:PRIMARY&gt; cfg = rs.conf()
{
        &quot;_id&quot; : &quot;replica1&quot;,
        &quot;version&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;host&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;arbiterOnly&quot; : false,
                        &quot;buildIndexes&quot; : true,
                        &quot;hidden&quot; : false,
                        &quot;priority&quot; : 1,
                        &quot;tags&quot; : {

                        },
                        &quot;slaveDelay&quot; : 0,
                        &quot;votes&quot; : 1
                }
        ],
        &quot;settings&quot; : {
                &quot;chainingAllowed&quot; : true,
                &quot;heartbeatTimeoutSecs&quot; : 10,
                &quot;getLastErrorModes&quot; : {

                },
                &quot;getLastErrorDefaults&quot; : {
                        &quot;w&quot; : 1,
                        &quot;wtimeout&quot; : 0
                }
        }
}
replica1:PRIMARY&gt; cfg.members[0].priority = 100
100
replica1:PRIMARY&gt; rs.reconfig(cfg)
{ &quot;ok&quot; : 1 }
</code></pre>
<p>Проверим состояние нашей реплики:</p>
<pre><code>replica1:PRIMARY&gt; rs.status()
{
        &quot;set&quot; : &quot;replica1&quot;,
        &quot;date&quot; : ISODate(&quot;2015-08-20T19:38:18.845Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 96,
                        &quot;optime&quot; : Timestamp(1440099489, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-08-20T19:38:09Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1440099465, 2),
                        &quot;electionDate&quot; : ISODate(&quot;2015-08-20T19:37:45Z&quot;),
                        &quot;configVersion&quot; : 2,
                        &quot;self&quot; : true
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre>
<p>Все ок. Ставим новый (пустой) сервер с mongo, прописываем в настройках replica set, запускаем. Теперь надо новый сервер добавить в набор:</p>
<pre><code>replica1:PRIMARY&gt; rs.add(&quot;db2.cluster:27017&quot;)
{ &quot;ok&quot; : 1 }
</code></pre>
<p>Узлы кластера должны быть доступны друг другу по порту, на котором работает mongo (по умолчанию 27017). Если используются имена, а не IP-адреса - важно убедится в том, что они правильно определяются со всех машин. Сервера общаются асинхронно, все со всеми. Проверим состояние нашего набора:</p>
<pre><code>replica1:PRIMARY&gt; rs.status()
{
        &quot;set&quot; : &quot;replica1&quot;,
        &quot;date&quot; : ISODate(&quot;2015-08-20T21:16:13.381Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 77,
                        &quot;optime&quot; : Timestamp(1440105350, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-08-20T21:15:50Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1440105297, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-08-20T21:14:57Z&quot;),
                        &quot;configVersion&quot; : 3,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;db2.cluster:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 5,
                        &quot;stateStr&quot; : &quot;STARTUP2&quot;,
                        &quot;uptime&quot; : 23,
                        &quot;optime&quot; : Timestamp(0, 0),
                        &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-08-20T21:16:12.351Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-08 20T21:16:12.363Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;syncingTo&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;configVersion&quot; : 3
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre>
<p>Статус STARTUP2 означает, что новый член набора синхронизируется (выгружает данные) с мастером. так, как по умолчанию приоритет у новых серверов - 1, даже после окончания, даже случайно этот сервер не сможет стать мастером, что убережет нас от потери данных.</p>
<p>Дождавшись синхронизации (state у нового сервера должен будет изменится на SECONDARY) - добавляем еще один сервер по точно такой же схеме:</p>
<pre><code>replica1:PRIMARY&gt; rs.add(&quot;db3.cluster:27017&quot;)
{ &quot;ok&quot; : 1 }
</code></pre>
<p>И снова ждем состояния SECONDARY. Теперь можно снизить приоритет основного сервера, чтобы он стал равноправным членом кластера:</p>
<pre><code>replica1:PRIMARY&gt; cfg.members[0].priority = 1
1
replica1:PRIMARY&gt; rs.reconfig(cfg)
{ &quot;ok&quot; : 1 }
</code></pre>
<p>И теперь убедимся, что все ок:</p>
<pre><code>replica1:PRIMARY&gt; rs.status()
{
        &quot;set&quot; : &quot;replica1&quot;,
        &quot;date&quot; : ISODate(&quot;2015-08-21T10:15:11.851Z&quot;),
        &quot;myState&quot; : 1,
        &quot;members&quot; : [
                {
                        &quot;_id&quot; : 0,
                        &quot;name&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 1,
                        &quot;stateStr&quot; : &quot;PRIMARY&quot;,
                        &quot;uptime&quot; : 46815,
                        &quot;optime&quot; : Timestamp(1440152107, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-08-21T10:15:07Z&quot;),
                        &quot;electionTime&quot; : Timestamp(1440105297, 1),
                        &quot;electionDate&quot; : ISODate(&quot;2015-08-20T21:14:57Z&quot;),
                        &quot;configVersion&quot; : 4,
                        &quot;self&quot; : true
                },
                {
                        &quot;_id&quot; : 1,
                        &quot;name&quot; : &quot;db2.cluster:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 46761,
                        &quot;optime&quot; : Timestamp(1440152107, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-08-21T10:15:07Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-08-21T10:15:09.856Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-08-21T10:15:09.997Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;syncingTo&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;configVersion&quot; : 4
                },
                {
                        &quot;_id&quot; : 2,
                        &quot;name&quot; : &quot;db3.cluster:27017&quot;,
                        &quot;health&quot; : 1,
                        &quot;state&quot; : 2,
                        &quot;stateStr&quot; : &quot;SECONDARY&quot;,
                        &quot;uptime&quot; : 46550,
                        &quot;optime&quot; : Timestamp(1440152107, 1),
                        &quot;optimeDate&quot; : ISODate(&quot;2015-08-21T10:15:07Z&quot;),
                        &quot;lastHeartbeat&quot; : ISODate(&quot;2015-08-21T10:15:09.856Z&quot;),
                        &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2015-08-21T10:15:09.997Z&quot;),
                        &quot;pingMs&quot; : 0,
                        &quot;syncingTo&quot; : &quot;db1.cluster:27017&quot;,
                        &quot;configVersion&quot; : 4
                }
        ],
        &quot;ok&quot; : 1
}
</code></pre>
</div>
<div class=post-info>
<div class="post-date dt-published">2015-09-06</div>
<a class="post-hidden-url u-url" href=https://prudnitskiy.pro/post/2015-09-06-mongo-shard/>https://prudnitskiy.pro/post/2015-09-06-mongo-shard/</a>
<a href=https://prudnitskiy.pro class="p-name p-author post-hidden-author h-card" rel=me>Paul Rudnitskiy</a>
<div class=post-taxonomies>
<ul class=post-tags>
<li><a href=https://prudnitskiy.pro/tags/mongo/>#mongo</a></li>
<li><a href=https://prudnitskiy.pro/tags/db/>#db</a></li>
</ul>
</div>
</div>
</article>
<div class="pagination post-pagination">
<div class="left pagination-item">
<a href=/post/2015-11-26-pinba/>Отслеживаем PHP с помощью PINBA на debian</a>
</div>
<div class="right pagination-item">
<a href=/post/2015-05-01-xtradb-quickstart/>Быстрая миграция MySQL на failover cluster</a>
</div>
</div>
</main>
<footer class=common-footer>
<div class=common-footer-bottom>
<div class=copyright>
<p>© Paul Rudnitskiy, 2023<br>
Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/mitrichius/hugo-theme-anubis>Anubis</a>.<br>
</p>
</div>
<button class=theme-switcher>
Dark theme
</button>
<script>const STORAGE_KEY='user-color-scheme',defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia('(prefers-color-scheme: dark)');const autoChangeScheme=a=>{currentTheme=a.matches?'dark':'light',document.documentElement.setAttribute('data-theme',currentTheme),changeButtonText()};document.addEventListener('DOMContentLoaded',function(){switchButton=document.querySelector('.theme-switcher'),currentTheme=detectCurrentScheme(),currentTheme=='dark'&&document.documentElement.setAttribute('data-theme','dark'),currentTheme=='auto'&&(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)),switchButton&&(changeButtonText(),switchButton.addEventListener('click',switchTheme,!1)),showContent()});function detectCurrentScheme(){return localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia?window.matchMedia('(prefers-color-scheme: dark)').matches?'dark':'light':'light'}function changeButtonText(){switchButton&&(switchButton.textContent=currentTheme=='dark'?"Light theme":"Dark theme")}function switchTheme(a){currentTheme=='dark'?(localStorage.setItem(STORAGE_KEY,'light'),document.documentElement.setAttribute('data-theme','light'),currentTheme='light'):(localStorage.setItem(STORAGE_KEY,'dark'),document.documentElement.setAttribute('data-theme','dark'),currentTheme='dark'),changeButtonText()}function showContent(){document.body.style.visibility='visible',document.body.style.opacity=1}</script>
</div>
<p class="h-card vcard">
<a href=https://prudnitskiy.pro class="p-name u-url url fn" rel=me>Paul Rudnitskiy</a>
<img class=u-photo src=/assets/avatars/ba.jpg>
</p>
</footer>
</div>
</body>
</html>