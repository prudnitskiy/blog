<!doctype html><html lang=ru-ru data-theme>
<head>
<meta charset=utf-8>
<meta name=HandheldFriendly content="True">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=referrer content="no-referrer-when-downgrade">
<title>Распределяем pod-ы по машинам в kubernetes - SRE Blog</title>
<meta name=description content="Как распределить POD-ы и не пожалеть об этом">
<link rel=icon type=image/x-icon href=https://prudnitskiy.pro/favicon.ico>
<link rel=apple-touch-icon-precomposed href=https://prudnitskiy.pro/favicon.png>
<style>body{visibility:hidden;opacity:0}</style>
<noscript>
<style>body{visibility:visible;opacity:1}</style>
</noscript>
<link rel=stylesheet href=https://prudnitskiy.pro/css/style.min.7805c73eb1fdd7219cadcad861e932566ed8a62e3c55386d0058c3967e8ab5d0.css integrity="sha256-eAXHPrH91yGcrcrYYekyVm7Ypi48VThtAFjDln6KtdA=">
<script src=https://prudnitskiy.pro/js/script.min.510c781c39dbb21b4c76d85c82e2bdf4689220adbb7cd61e49e9d293e442fb42.js type=text/javascript integrity="sha256-UQx4HDnbshtMdthcguK99GiSIK27fNYeSenSk+RC+0I="></script>
<meta property="og:title" content="Распределяем pod-ы по машинам в kubernetes">
<meta property="og:description" content="Как распределить POD-ы и не пожалеть об этом">
<meta property="og:type" content="article">
<meta property="og:url" content="https://prudnitskiy.pro/post/2021-01-15-k8s-pod-distribution/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2021-01-15T08:16:22+00:00">
<meta property="article:modified_time" content="2021-01-15T08:16:22+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Распределяем pod-ы по машинам в kubernetes">
<meta name=twitter:description content="Как распределить POD-ы и не пожалеть об этом">
</head>
<body>
<a class=skip-main href=#main>Skip to main content</a>
<div class=container>
<header class=common-header>
<div class=header-top>
<h1 class=site-title>
<a href=/>SRE Blog</a>
</h1>
<ul class=social-icons>
<li>
<a href=https://github.com/prudnitskiy title=Github rel=me>
<span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span>
</a>
</li>
<li>
<a href=https://www.linkedin.com/in/prudnitskiy title=Linkedin rel=me>
<span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span>
</a>
</li>
</ul>
</div>
<nav>
<a href=https://prudnitskiy.pro/about/ title=About>About</a>
<a href=https://prudnitskiy.pro/ title=Blog>Blog</a>
</nav>
</header>
<main id=main tabindex=-1>
<article class="post h-entry">
<div class=post-header>
<header>
<h1 class="p-name post-title">Распределяем pod-ы по машинам в kubernetes</h1>
</header>
</div>
<div class="content e-content">
<p>Kubernetes иногда называют &ldquo;операционной системой для дата-центров&rdquo; &ndash; и в этом есть логика. K8s позволяет представить группу серверов (условный ЦОД) как единое вычислительное пространство. Оператор просто бросает задания в K8s, а тот сам выбирает, где тот или иной контейнер лучше разместить. Чаще всего делает это он хорошо. Но иногда появляется необходимость как-то управлять этим процессом. Об этом я и расскажу.</p>
<h2 id=зачем-управлять-распределением-pod-ов>Зачем управлять распределением POD-ов?
<span>
<a href=#%d0%b7%d0%b0%d1%87%d0%b5%d0%bc-%d1%83%d0%bf%d1%80%d0%b0%d0%b2%d0%bb%d1%8f%d1%82%d1%8c-%d1%80%d0%b0%d1%81%d0%bf%d1%80%d0%b5%d0%b4%d0%b5%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5%d0%bc-pod-%d0%be%d0%b2><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h2><p>Зачем вообще нужно привязывать поды к определенным узлам? Это может быть связано с производительностью, безопасностью или надежность. Например &ndash; pod может требовать доступ к специфическому железу (видеокарты и ML-ускорители для задач машинного обучения, аппаратные криптоускорители). Это может быть продиктовано безопасностью: критические части проекта будут размещаться на машинах, где физически не может быть ничего, кроме них. Это снижает шансы на то, что удачный взлом, скажем, сервиса регистраций раскроет данные о платежах. Некоторые стандарты безопасности (включая PCI DSS) имеют даже требования к физической безопасности серверов &ndash; датчики вскрытия, пломбы на корпусках, запрет на доступ. Отдельная удобная особенность &ndash; tier-инг. Нагрузку в кластере можно разделить на &ldquo;важную&rdquo; и &ldquo;не очень&rdquo;. Под важную выделять мощные современные машины с резервированием PSU, горячей замены дисков и памяти, под &ldquo;не очень&rdquo; &ndash; соскрести какой-нибудь хлам. В облаках это делается даже проще за счет spot instances. Такие инстансы дешевле (порой радикально), но их работу никто не гарантирует &ndash; инстанс может отключится в любой момент (вместо него появится новый). Это вызовет пересоздание POD-ов, но для чего-то маловажного это, может &ndash; и не страшно совсем.</p>
<h2 id=способы-управления>Способы управления
<span>
<a href=#%d1%81%d0%bf%d0%be%d1%81%d0%be%d0%b1%d1%8b-%d1%83%d0%bf%d1%80%d0%b0%d0%b2%d0%bb%d0%b5%d0%bd%d0%b8%d1%8f><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h2><h3 id=nodeselector>NodeSelector
<span>
<a href=#nodeselector><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h3><p>Это самый простой способ управления аллокацией. Он предельно прямолинеен &ndash; запутаться в нем невозможно. Выполняется в 2 этапа. Сначала надо поставить метки на node командой label:</p>
<pre><code>kubectl label nodes snowflake3 disk=hdd
</code></pre>
<p>Проверить, какие метки уже есть можно через <code>kubectl describe nodes</code></p>
<p>Теперь можно указать pod-у требование на привязку к конкретной метке. Для аллокации пода будут использоваться только помеченые узлы, то есть при включении nodeSelector для пода ноды без меток будут игнорироваться:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disk: hdd
</code></pre>
<p>Для deployment nodeSelector передается в шаблон pod-а, как обычно. Не смотря на удобство и прямолинейность подхода &ndash; nodeSelector имеет три минуса:</p>
<ul>
<li>nodeSelector применяется в момент аллокации пода и бесполезен, если под уже аллоцирован. Если вам нужно &ldquo;освободить&rdquo; ноду &ndash; придется поставить на нее метку и затем выкинуть оттуда поды командой <code>drain</code></li>
<li>nodeSelector не особенно гибкий и работает по принципу &ldquo;один к одному&rdquo;. К примеру, можно сделать метки для машин <code>small</code>, <code>medium</code> и <code>large</code> и указать поду, что он должен развернуться на машине класса <code>small</code>. Но нельзя &ndash; на машине класса <code>medium</code> или <code>large</code> &ndash; возможен только один вариант.</li>
<li>nodeSelector не запрещает аллокаций. То есть на машине с меткой могут размещаться поды без nodeAffinity. Для решения этой проблемы придуман иной подход.</li>
</ul>
<h3 id=taints-and-tolerations>Taints and Tolerations
<span>
<a href=#taints-and-tolerations><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h3><p>Taints &ndash; это NodeAffinity наоборот. Если nodeAffinity говорит scheduler-у, где он должен размещать pod-ы, то taint говорит, где pod-ы размещать <em>нельзя</em>. Любой taint запрещает размещение на машине любых подов (есть одно исключение, про него дальше). Однако можно создать под, который будет игнорировать (<code>tolerate</code>) этот запрет &ndash; и данный pod запустится на данной машине. Даже если у вас есть совершенно пустой нормальный кластер kubernetes &ndash; у вас уже есть taint. По умолчанию kubernetes запрещает размещать обычные поды на master nodes &ndash; это taint <code>node-role.kubernetes.io/master</code></p>
<p>taint создается с помощью команды <code>kubectl taint</code>. Общий вид:</p>
<pre><code>kubectl taint nodes nodeName taintKey=taintValue:taintEffect
</code></pre>
<p>taintKey и taintValue &ndash; это просто метки, они могут быть произвольными. У taintEffect есть 3 возможных значения:</p>
<ul>
<li>NoSchedule &ndash; новые поды не будут аллоцироваться, однако существующие продолжат свою работу</li>
<li>PreferNoSchedule &ndash; новые поды не будут аллоцироваться, если в кластере есть свободное место</li>
<li>NoExecute &ndash; все запущенные поды без tolerations должны быть убраны</li>
</ul>
<p>Теперь о том, как прописываются tolerations. Язык tolerations слегка сложнее прямолинейного подхода nodeAffinity:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: &quot;pft-env&quot;
    operator: &quot;Exists&quot;
    effect: &quot;NoSchedule&quot;
</code></pre>
<p>в данном примере мы создадим под, который будет игнорировать taint, созданный вот такой командой:</p>
<pre><code>kubectl taint nodes pft-node-1 pft-env=true:NoSchedule
</code></pre>
<p>Есть более сложный вариант &ndash; можно учитывать не только факт наличия метки, но и ее значение. Создадим пару taint-ов:</p>
<pre><code>kubectl taint nodes secure-1 secGroup=secure:NoSchedule
kubectl taint nodes insecure-2 secGroup=unsafe:NoSchedule

apiVersion: v1
kind: Pod
metadata:
  name: vault
  labels:
    env: test
spec:
  containers:
  - name: vault
    image: vault
  tolerations:
  - key: &quot;secGroup&quot;
    operator: &quot;Equals&quot;
    value: &quot;secure&quot;
    effect: &quot;NoSchedule&quot;
</code></pre>
<p>В данном примере pod vault будет создан только на ноде secure-1, потому что только на ней <code>secGroup</code> равен <code>secure</code>.</p>
<p>Taint-ов можно создать сколь угодно много и условия проверки можно сочетать, как в примере ниже:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: processing
  labels:
    env: test
spec:
  containers:
  - name: processing
    image: processing
  tolerations:
  - key: &quot;dedicatedNode&quot;
    operator: &quot;Exists&quot;
    effect: &quot;NoSchedule&quot;
  - key: &quot;secGroup&quot;
    operator: &quot;Equals&quot;
    value: &quot;secure&quot;
    effect: &quot;NoExecute&quot;
</code></pre>
<p>В данном примере мы выделяем пул выделенных машин taint-ом &ldquo;dedicatedNode&rdquo; и отдельно помечаем группу максимальной безопасности значением secure для группы secGroup.</p>
<p>Удалить taint можно, добавив в конец знак минуса:</p>
<pre><code>kubectl taint nodes secure-1 secGroup=secure:NoSchedule-
</code></pre>
<h3 id=nodeaffinity>nodeAffinity
<span>
<a href=#nodeaffinity><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h3><p>Не смотря на простоту и эффективность механизма nodeSelector &ndash; механизм это прямолинейный и не особенно гибкий. Авторы kubernetes предлагают более мощный, гибкий (а так же &ndash; сложный и неудобный) механизм &ndash; nodeAffinity. Язык описания nodeAffinity предлагает несколько мощных возомжностей:</p>
<ul>
<li>логические операторы для выбора условия размещения &ndash; IN (размещать на одной из нод с разными метками) или AND (размещать на нодах, имеющих обе метки сразу)</li>
<li>можно выбраить политики размещения pod-ов относительно друг друга: например &ndash; запретить экземплярам кэша оказываться на одной физической машине или требовать размещение приложения вместе с экземпляром кеша на одном физическом узле</li>
</ul>
<p>Минус nodeAffinity в том, что язык очень многословный и читается тяжело. Общая спецификация выглядит так:</p>
<pre><code>spec:
  affinity:
    nodeAffinity:
      {affinityClass}:
        nodeSelectorTerms:
        - matchExpressions:
          - key: {affinityKey}
            operator: {affinityOperator}
            values:
            - {affinityValues}
</code></pre>
<p>affinityClass влияет на строгость выбора узла:</p>
<ul>
<li>requiredDuringSchedulingIgnoredDuringExecution: обязательно размещать pod-ы по требованию nodeAffinity. Если разместить не получится &ndash; pod застрянет в статусе Pending</li>
<li>preferredDuringSchedulingIgnoredDuringExecution: по возможности размещать pod-ы по требованиям affinity. Если поды не влезли &ndash; scheduler разместит их &ldquo;как получится&rdquo;</li>
</ul>
<p>affinityKey &ndash; это метка (ключ), по которой мы будем искать ноды для размещения pod-ов.
affinityValues &ndash; это значения метки, которые нам подойдут
affinityOperator &ndash; это тот логический оператор, по которому будет производится выбор метки. Варианты:</p>
<ul>
<li>In &ndash; подойдет любое из перечисленных значений</li>
<li>NotIn &ndash; противоположно In</li>
<li>Exists &ndash; метка просто есть (values игнорируется)</li>
<li>DoesNotExists &ndash; противоположно Exists</li>
<li>Gt &ndash; Greater than &ndash; значение метки больше указанного в политике числа. Сработает только для чисел</li>
<li>Lt &ndash; Less than &ndash; противоположно Gt</li>
</ul>
<p>affinityClass preferredDuringSchedulingIgnoredDuringExecution слегка отличается &ndash; вместо nodeSelectorTerms используется поле preference (синтаксис такой же), плюс есть обязательное поле weight &ndash; оно отвечает за приоритет при выборе node.</p>
<p>Пример:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/e2e-az-name
            operator: In
            values:
            - e2e-az1
            - e2e-az2
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: kubernetes.io/node-tier
            operator: In
            values:
            - silver
            - bronze
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/nginx
</code></pre>
<p>affinity не учитывается для уже аллоцированных nodes, так что если нужно освободить node-у от всех подов которые там уже есть &ndash; поможет команда <code>kubectl node drain</code></p>
<h4 id=лирическое-отступление----podaffinitty-и-podantiaffinitty>Лирическое отступление &ndash; PodAffinitty и PodAntiAffinitty
<span>
<a href=#%d0%bb%d0%b8%d1%80%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%be%d0%b5-%d0%be%d1%82%d1%81%d1%82%d1%83%d0%bf%d0%bb%d0%b5%d0%bd%d0%b8%d0%b5----podaffinitty-%d0%b8-podantiaffinitty><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h4><p>Механизм, который помогает размещать pod-ы относительно нод &ndash; может так же помочь и разместить pod-ы относительно друг друга &ndash; за это отвечают классы PodAffinity и PodAntiAffinity. Все три класса можно сочетать друг с другом, синтаксис внутри одинаковый, по этому просто покажу пример:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: &quot;kubernetes.io/hostname&quot;
      containers:
      - name: redis-server
        image: redis:3.2-alpine
</code></pre>
<p>В этом примере мы запрещаем экземплярам redis размещаться на одном узле. Каждый pod в этом deployment получит метку <code>app:store</code>, политика podAntiAffinity запрещает размещать второй под с меткой app=store на ноде с таким же hostname. Важный параметр тут &ndash; topologyKey. Именно по нему scheduler понимает, какие node-ы считаются одной зоной размещения,а какие &ndash; нет. Усложним пример, добавив web worker:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: &quot;kubernetes.io/hostname&quot;
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: &quot;kubernetes.io/hostname&quot;
      containers:
      - name: web-app
        image: nginx:1.16-alpine
</code></pre>
<p>В этом примере мы размещаем nginx на разных node (как мы сделали с redis), но при этом требуем, чтобы nginx размещался вместе с redis. Это может быть удобно для кешей. Проверим, что получилось:</p>
<pre><code>&gt; kubectl get pods -o wide

NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
redis-cache-1450370735-6dzlj   1/1       Running   0          8m        10.192.4.2   kube-node-3
redis-cache-1450370735-j2j96   1/1       Running   0          8m        10.192.2.2   kube-node-1
redis-cache-1450370735-z73mh   1/1       Running   0          8m        10.192.3.1   kube-node-2
web-server-1287567482-5d4dz    1/1       Running   0          7m        10.192.2.3   kube-node-1
web-server-1287567482-6f7v5    1/1       Running   0          7m        10.192.4.3   kube-node-3
web-server-1287567482-s330j    1/1       Running   0          7m        10.192.3.2   kube-node-2
</code></pre>
<h3 id=static-pod-allocations>Static pod allocations
<span>
<a href=#static-pod-allocations><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h3><p>Это очень редкий случай, но не упомянуть его было бы нечестно. Pod-ы можно аллоцировать полностью статически, вручную привязав к конкретной node. В этом случае scheduler никак на них не влияет. На них не действуют taints, nodeSelector и podAffinity. Даже node drain ничего не сможет с такими подами сделать. Зачем это может потребоваться? Ну, во-первых для запуска таких pod-ов не нужен работающий scheduler или apiserver. Это делает размещение таких подов сверхнадежным &ndash; они будут работать всегда. Именно так kubeadm устанавливает свои компоненты &ndash; это не полноценные демоны, а контейнеры, которые вручную привязаны к master node.</p>
<p>Во-вторых такой подход может потребоваться в случае, если какой-то контейнер надо привязать к конкретной, строго определенной node вручную и ни при каких условиях не давать ему оттуда уезжать. Скажем, у вас какое-то особое шифрование и оно зависит от HSM, который физически подключен к определенной, особо защищенной машине. Вообще &ndash; это порочная практика и такой сценарий лучше решается через nodeSelector + taint, но мало ли?</p>
<p>Выполнить статическую аллоакцию очень просто &ndash; нужно положить манифесты pod-ов в папку со статическими подами. Этот путь можно задать двумя путями:</p>
<ul>
<li>через аргумент командной строки kubelet: <code>--pod-manifest-path</code></li>
<li>через параметр конфига <code>staticPodPath</code></li>
</ul>
<p>Если у вас kubernetes установлен через kubeadm &ndash; этот параметр там уже есть, kubelet будет искать статические манифесты по адресу <code>/etc/kubernetes/manifests</code>. Kubelet перечитывает папку с манифестами каждые 10 секунд. Если удалить манифест &ndash; kubernetes удалит pod.</p>
<p>Просто создадим манифест статического пода</p>
<pre><code>cat &lt;&lt;EOF &gt;/etc/kubernetes/manifests/static-web.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-web
  labels:
    role: static
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
EOF
</code></pre>
<p>И проверим, что получилось:</p>
<pre><code>&gt; kubectl get pods -l role=static

NAME                       READY     STATUS    RESTARTS   AGE
static-web-my-node1        1/1       Running   0          2m
</code></pre>
<h2 id=порядок-применения>Порядок применения
<span>
<a href=#%d0%bf%d0%be%d1%80%d1%8f%d0%b4%d0%be%d0%ba-%d0%bf%d1%80%d0%b8%d0%bc%d0%b5%d0%bd%d0%b5%d0%bd%d0%b8%d1%8f><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h2><p>Первым всегда применяется static pod. Он игнорирует все (taints, affinities, node selectors).</p>
<p>Вторым по списку применяется taint. Если у pod нет toleration &ndash; он не будет размещен, по этому taint &ndash; это очень эффективный способ &ldquo;разогнать&rdquo; pod-ы с определенного узла (или группы узлов).</p>
<p>В случае, если есть nodeAffinity и nodeSelector &ndash; должны сработать оба условия сразу (то есть &ndash; и метка селектора и условия affinity).</p>
<h2 id=заключение>Заключение
<span>
<a href=#%d0%b7%d0%b0%d0%ba%d0%bb%d1%8e%d1%87%d0%b5%d0%bd%d0%b8%d0%b5><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg>
</a>
</span>
</h2><p>Kubernetes &ndash; мощный, богатый на возможности инструмент. Он кажется слегка неудобным, но ровно до момента понимания логики его работы. Scheduler у kubernetes практически ключевой компонент, и он достаточно гибок, пусть и не самым лучшим образом описан. Надеюсь &ndash; эта статья кому-то поможет. Высокого вам аптайма!</p>
</div>
<div class=post-info>
<div class="post-date dt-published">2021-01-15</div>
<a class="post-hidden-url u-url" href=https://prudnitskiy.pro/post/2021-01-15-k8s-pod-distribution/>https://prudnitskiy.pro/post/2021-01-15-k8s-pod-distribution/</a>
<a href=https://prudnitskiy.pro class="p-name p-author post-hidden-author h-card" rel=me>Paul Rudnitskiy</a>
<div class=post-taxonomies>
<ul class=post-tags>
<li><a href=https://prudnitskiy.pro/tags/k8s/>#k8s</a></li>
</ul>
</div>
</div>
</article>
<div class="pagination post-pagination">
<div class="left pagination-item">
<a href=/post/2022-02-16-CKA/>Как сдать экзамен Cerified Kubernetes Administrator: личный опыт</a>
</div>
<div class="right pagination-item">
<a href=/post/2020-06-23-restic-quickstart/>Restic: backup для современного мира</a>
</div>
</div>
</main>
<footer class=common-footer>
<div class=common-footer-bottom>
<div class=copyright>
<p>© Paul Rudnitskiy, 2023<br>
Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/mitrichius/hugo-theme-anubis>Anubis</a>.<br>
</p>
</div>
<button class=theme-switcher>
Dark theme
</button>
<script>const STORAGE_KEY='user-color-scheme',defaultTheme="auto";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia('(prefers-color-scheme: dark)');const autoChangeScheme=a=>{currentTheme=a.matches?'dark':'light',document.documentElement.setAttribute('data-theme',currentTheme),changeButtonText()};document.addEventListener('DOMContentLoaded',function(){switchButton=document.querySelector('.theme-switcher'),currentTheme=detectCurrentScheme(),currentTheme=='dark'&&document.documentElement.setAttribute('data-theme','dark'),currentTheme=='auto'&&(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)),switchButton&&(changeButtonText(),switchButton.addEventListener('click',switchTheme,!1)),showContent()});function detectCurrentScheme(){return localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia?window.matchMedia('(prefers-color-scheme: dark)').matches?'dark':'light':'light'}function changeButtonText(){switchButton&&(switchButton.textContent=currentTheme=='dark'?"Light theme":"Dark theme")}function switchTheme(a){currentTheme=='dark'?(localStorage.setItem(STORAGE_KEY,'light'),document.documentElement.setAttribute('data-theme','light'),currentTheme='light'):(localStorage.setItem(STORAGE_KEY,'dark'),document.documentElement.setAttribute('data-theme','dark'),currentTheme='dark'),changeButtonText()}function showContent(){document.body.style.visibility='visible',document.body.style.opacity=1}</script>
</div>
<p class="h-card vcard">
<a href=https://prudnitskiy.pro class="p-name u-url url fn" rel=me>Paul Rudnitskiy</a>
<img class=u-photo src=/assets/avatars/ba.jpg>
</p>
</footer>
</div>
</body>
</html>